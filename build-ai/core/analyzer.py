# core/analyzer.py
import json
import re
from typing import Dict, List, Optional
from openai import OpenAI
from datetime import datetime
import hashlib
from pathlib import Path
from .prompts import PromptManager
from config.settings import REPORT_CONFIG  # å¯¼å…¥é…ç½®

class BuildErrorAnalyzer:
    """æ„å»ºé”™è¯¯åˆ†æå™¨"""
    
    def __init__(self, deepseek_config: Dict, knowledge_base):
        
        # ç›´æ¥ä»é…ç½®è·å–æŠ¥å‘Šç›®å½•
        self.reports_dir = REPORT_CONFIG["output_dir"]
        
        print(f"ğŸ“ æŠ¥å‘Šç›®å½•: {self.reports_dir}")
        print(f"ğŸ” æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨: {self.reports_dir.exists()}")

        self.client = OpenAI(
            api_key=deepseek_config["api_key"],
            base_url=deepseek_config["base_url"]
        )
        self.model = deepseek_config["model"]
        self.temperature = deepseek_config["temperature"]
        self.max_tokens = deepseek_config["max_tokens"]
        
        self.knowledge_base = knowledge_base
        self.prompt_manager = PromptManager()
        
        # é”™è¯¯æ¨¡å¼è¯†åˆ«
        self.error_patterns = self._init_error_patterns()
    
    def _init_error_patterns(self) -> Dict:
        """åˆå§‹åŒ–é”™è¯¯æ¨¡å¼"""
        return {
            "dependency": [
                r"npm ERR!", r"yarn error", r"pip install failed",
                r"Could not resolve dependency", r"Package.*not found",
                r"ä¾èµ–.*å¤±è´¥", r"ä¸‹è½½.*å¤±è´¥"
            ],
            "permission": [
                r"Permission denied", r"EACCES", r"æƒé™ä¸å¤Ÿ",
                r"access denied", r"æ— æƒè®¿é—®", r"Forbidden"
            ],
            "resource": [
                r"No space left", r"å†…å­˜ä¸è¶³", r"disk full",
                r"OutOfMemoryError", r"å†…å­˜æº¢å‡º", r"èµ„æºä¸è¶³"
            ],
            "configuration": [
                r"Configuration error", r"é…ç½®é”™è¯¯", 
                r"Invalid configuration", r"Missing.*property",
                r"å‚æ•°é”™è¯¯", r"é…ç½®æ–‡ä»¶"
            ],
            "network": [
                r"Connection refused", r"Timeout", r"ç½‘ç»œé”™è¯¯",
                r"Failed to connect", r"è¿æ¥å¤±è´¥", r"è¯·æ±‚è¶…æ—¶"
            ],
            "code": [
                r"SyntaxError", r"ç¼–è¯‘é”™è¯¯", r"syntax error",
                r"undefined variable", r"ç±»å‹é”™è¯¯", r"ç¼–è¯‘å¤±è´¥"
            ]
        }
    
    def analyze_error_log(self, log_content: str, log_source: str = "unknown") -> Dict:
        """åˆ†æé”™è¯¯æ—¥å¿—"""
        print(f"ğŸ” å¼€å§‹åˆ†æé”™è¯¯æ—¥å¿—: {log_source}")
        
        # 1. æå–å…³é”®é”™è¯¯ä¿¡æ¯
        error_snippets = self._extract_error_snippets(log_content)
        print(f"ğŸ“ æå–åˆ° {len(error_snippets)} ä¸ªé”™è¯¯ç‰‡æ®µ")
        
        # 2. æ£€ç´¢ç›¸å…³çŸ¥è¯†,ä¼šæ‹¼æ¥é”™è¯¯å†…å®¹ï¼Œç„¶åä¸€å¥è¯çš„æ–¹å¼å»å‘é‡æ•°æ®åº“åŒ¹é…
        query_text = self._build_query_from_snippets(error_snippets)
        knowledge_results = self.knowledge_base.search(query_text, top_k=3)
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context = self._format_knowledge_context(knowledge_results)
        
        # 4. è°ƒç”¨AIåˆ†æ
        analysis_result = self._call_ai_analysis(
            log_content[:2000],  # é™åˆ¶é•¿åº¦
            context
        )

        # 5. å¢å¼ºç»“æœ
        enhanced_result = self._enhance_analysis_result(
            analysis_result, 
            error_snippets,
            knowledge_results
        )
        
        # 6. ä¿å­˜åˆ†æè®°å½•
        self._save_analysis_record(enhanced_result, log_source)
        
        return enhanced_result
    
    def _extract_error_snippets(self, log_content: str) -> List[str]:
        """æå–å…³é”®é”™è¯¯ç‰‡æ®µ"""
        snippets = []
        lines = log_content.split('\n')
        
        for i, line in enumerate(lines):
            for error_type, patterns in self.error_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        # æå–ä¸Šä¸‹æ–‡ï¼ˆå‰å2è¡Œï¼‰
                        start = max(0, i - 2)
                        end = min(len(lines), i + 3)
                        snippet = '\n'.join(lines[start:end])
                        snippets.append({
                            "content": snippet,
                            "error_type": error_type,
                            "line_number": i + 1
                        })
                        break
        
        # å»é‡
        unique_snippets = []
        seen = set()
        for snippet in snippets:
            content_hash = hashlib.md5(snippet["content"].encode()).hexdigest()[:8]
            if content_hash not in seen:
                seen.add(content_hash)
                unique_snippets.append(snippet)
        
        return unique_snippets[:5]  # æœ€å¤šè¿”å›5ä¸ª
    
    def _build_query_from_snippets(self, snippets: List[Dict]) -> str:
        """ä»é”™è¯¯ç‰‡æ®µæ„å»ºæŸ¥è¯¢"""
        if not snippets:
            return "æ„å»ºé”™è¯¯"
        
        # ä½¿ç”¨é”™è¯¯ç±»å‹å’Œå†…å®¹æ„å»ºæŸ¥è¯¢
        error_types = set(s["error_type"] for s in snippets)
        query_parts = []
        
        # æ·»åŠ é”™è¯¯ç±»å‹
        query_parts.append(" ".join(error_types))
        
        # æ·»åŠ å…³é”®å†…å®¹ï¼ˆæ¯æ®µå–å‰50å­—ç¬¦ï¼‰
        for snippet in snippets[:2]:
            content_preview = snippet["content"][:50].replace('\n', ' ')
            query_parts.append(content_preview)
        
        return " ".join(query_parts)
    
    def _format_knowledge_context(self, knowledge_results: List[Dict]) -> str:
        """æ ¼å¼åŒ–çŸ¥è¯†ä¸Šä¸‹æ–‡"""
        if not knowledge_results:
            return "æš‚æ— ç›¸å…³çŸ¥è¯†"
        
        context_parts = ["æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³è§£å†³æ–¹æ¡ˆï¼š"]
        for i, result in enumerate(knowledge_results):
            context_parts.append(
                f"\nã€è§£å†³æ–¹æ¡ˆ {i+1} - ç›¸ä¼¼åº¦:{result['similarity']:.2f}ã€‘\n"
                f"{result['content']}"
            )
        
        return "\n".join(context_parts)
    
    def _call_ai_analysis(self, error_log: str, context: str) -> Dict:
        """è°ƒç”¨AIè¿›è¡Œåˆ†æ"""
        try:
            prompt = self.prompt_manager.get_analysis_prompt(error_log, context)
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯åä¸ºäº‘ç¼–è¯‘æ„å»ºä¸“å®¶"},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                response_format={"type": "json_object"}
            )
            
            result_text = response.choices[0].message.content
            return json.loads(result_text)
            
        except Exception as e:
            print(f"âŒ AIåˆ†æå¤±è´¥: {str(e)}")
            return {
                "error_summary": "AIåˆ†æå¤±è´¥",
                "error_type": "other",
                "root_cause": f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}",
                "confidence": 0.0,
                "fix_steps": [],
                "verification": "",
                "prevention": ""
            }
    
    def _enhance_analysis_result(self, analysis_result: Dict, 
                                error_snippets: List[Dict],
                                knowledge_results: List[Dict]) -> Dict:
        """å¢å¼ºåˆ†æç»“æœ"""
        enhanced = analysis_result.copy()
        
        # æ·»åŠ åŸå§‹æ•°æ®
        enhanced["error_snippets"] = error_snippets
        enhanced["knowledge_references"] = [
            {
                "content": r["content"][:100] + "...",
                "similarity": r["similarity"]
            }
            for r in knowledge_results
        ]
        
        # æ·»åŠ æ—¶é—´æˆ³
        enhanced["analyzed_at"] = datetime.now().isoformat()
        
        # è®¡ç®—ç½®ä¿¡åº¦è°ƒæ•´
        if knowledge_results:
            # æœ‰ç›¸å…³çŸ¥è¯†ï¼Œæé«˜ç½®ä¿¡åº¦
            max_similarity = max(r["similarity"] for r in knowledge_results)
            confidence_boost = min(0.2, max_similarity * 0.3)
            enhanced["confidence"] = min(1.0, enhanced.get("confidence", 0.5) + confidence_boost)
        
        return enhanced
    
    def _save_analysis_record(self, result: Dict, log_source: str):
        """ä¿å­˜åˆ†æè®°å½•"""
        record = {
            "log_source": log_source,
            "analyzed_at": result["analyzed_at"],
            "error_summary": result["error_summary"],
            "error_type": result["error_type"],
            "confidence": result["confidence"]
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        record_file = f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        record_path = self.reports_dir / record_file
        
        with open(record_path, 'w', encoding='utf-8') as f:
            json.dump(record, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ åˆ†æè®°å½•å·²ä¿å­˜: {record_path}")